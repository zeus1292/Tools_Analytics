{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "patha =  r'/Users/sanjayshah/Desktop/Tools_Analytics-master/df_processed.csv'\n",
    "with open(patha):\n",
    "    df=pd.read_csv(patha)\n",
    "    \n",
    "\n",
    "df = df.dropna(how='any',axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_data(df):\n",
    "    \n",
    "    #This list stores all the categories\n",
    "    category_list=list()\n",
    "\n",
    "    \n",
    "    #Assign categories to each book\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if df['Rating'].iloc[i]>='4.5':\n",
    "            category_list.append(1)\n",
    "        elif df['Rating'].iloc[i]<'4.5' and df['Rating'].iloc[i]>='4':\n",
    "            category_list.append(2)\n",
    "        elif df['Rating'].iloc[i]<'4' and df['Rating'].iloc[i]>='3.5':\n",
    "            category_list.append(3)\n",
    "        elif df['Rating'].iloc[i]<'3.5' and df['Rating'].iloc[i]>='3':\n",
    "            category_list.append(4)\n",
    "        elif df['Rating'].iloc[i]<'3' and df['Rating'].iloc[i]>='2.5':\n",
    "            category_list.append(5)\n",
    "        else:\n",
    "            category_list.append(0)\n",
    "\n",
    "    #Add categories to the DF\n",
    "    df['Category']=category_list\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=categorize_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjayshah/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['', \"It's\", 'Christmas', 'Eve', 'and', 'the', 'JOLLY', 'POSTMAN', 'is', 'delivering', 'greetings', 'to', 'various', 'fairy-tale', 'characters', '-', \"there's\", 'a', 'card', 'for', 'Baby', 'Bear,', 'a', 'game', 'appropriately', 'called', \"'Beware'\", 'for', 'Red', 'Riding', 'Hood', 'from', 'Mr', 'Wolf,', 'a', 'get-well', 'jigsaw', 'for', 'hospitalised', 'Humpty', 'Dumpty', 'and', 'three', 'more', 'surprise', 'envelopes', 'containing', 'letters,', 'cards,', 'etc.', \"Everyone's\", 'favourite', 'postman', 'keeps', 'on', 'peddling', 'his', 'bicycle', 'up', 'hill', 'and', 'down', 'dale', '.', '.', '.', 'and', 'into', \"everybody's\", 'hearts.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['christma', 'jolli', 'postman', 'deliv', 'greet', 'fairi', 'tale', 'charact', 'card', 'babi', 'bear', 'game', 'appropri', 'call', 'bewar', 'rid', 'hood', 'wolf', 'jigsaw', 'hospitalis', 'humpti', 'dumpti', 'surpris', 'envelop', 'contain', 'letter', 'card', 'favourit', 'postman', 'keep', 'peddl', 'bicycl', 'hill', 'dale', 'everybodi', 'heart']\n",
      "0 account\n",
      "1 america\n",
      "2 american\n",
      "3 amus\n",
      "4 author\n",
      "5 backroom\n",
      "6 bayous\n",
      "7 bestsel\n",
      "8 bill\n",
      "9 blackmail\n",
      "10 blow\n",
      "[(66, 2), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 2), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 2), (110, 1), (111, 5), (112, 1), (113, 9), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1)]\n",
      "Word 66 (\"success\") appears 2 time.\n",
      "Word 80 (\"approach\") appears 1 time.\n",
      "Word 81 (\"break\") appears 1 time.\n",
      "Word 82 (\"busi\") appears 1 time.\n",
      "Word 83 (\"chang\") appears 1 time.\n",
      "Word 84 (\"compani\") appears 1 time.\n",
      "Word 85 (\"despit\") appears 1 time.\n",
      "Word 86 (\"eat\") appears 1 time.\n",
      "Word 87 (\"economi\") appears 1 time.\n",
      "Word 88 (\"essenti\") appears 1 time.\n",
      "Word 89 (\"experi\") appears 1 time.\n",
      "Word 90 (\"expert\") appears 1 time.\n",
      "Word 91 (\"explain\") appears 1 time.\n",
      "Word 92 (\"fill\") appears 1 time.\n",
      "Word 93 (\"grant\") appears 1 time.\n",
      "Word 94 (\"great\") appears 1 time.\n",
      "Word 95 (\"guarante\") appears 1 time.\n",
      "Word 96 (\"handl\") appears 1 time.\n",
      "Word 97 (\"healthi\") appears 1 time.\n",
      "Word 98 (\"kind\") appears 1 time.\n",
      "Word 99 (\"know\") appears 1 time.\n",
      "Word 100 (\"learn\") appears 1 time.\n",
      "Word 101 (\"life\") appears 2 time.\n",
      "Word 102 (\"master\") appears 1 time.\n",
      "Word 103 (\"necessari\") appears 1 time.\n",
      "Word 104 (\"negat\") appears 1 time.\n",
      "Word 105 (\"overcom\") appears 1 time.\n",
      "Word 106 (\"posit\") appears 1 time.\n",
      "Word 107 (\"principl\") appears 1 time.\n",
      "Word 108 (\"product\") appears 1 time.\n",
      "Word 109 (\"reject\") appears 2 time.\n",
      "Word 110 (\"reluct\") appears 1 time.\n",
      "Word 111 (\"sale\") appears 5 time.\n",
      "Word 112 (\"season\") appears 1 time.\n",
      "Word 113 (\"sell\") appears 9 time.\n",
      "Word 114 (\"situat\") appears 1 time.\n",
      "Word 115 (\"stay\") appears 1 time.\n",
      "Word 116 (\"teach\") appears 1 time.\n",
      "Word 117 (\"techniqu\") appears 1 time.\n",
      "Word 118 (\"treat\") appears 1 time.\n",
      "Word 119 (\"turn\") appears 1 time.\n",
      "[(0, 0.06561659825470399),\n",
      " (1, 0.05674769278149435),\n",
      " (2, 0.05051423770781368),\n",
      " (3, 0.03360967924793841),\n",
      " (4, 0.08076942236832206),\n",
      " (5, 0.10361026494167715),\n",
      " (6, 0.07326835510094977),\n",
      " (7, 0.10975620931987545),\n",
      " (8, 0.1100888385863401),\n",
      " (9, 0.10557863727426536),\n",
      " (10, 0.1141456719262901),\n",
      " (11, 0.11566921939688658),\n",
      " (12, 0.10887796843398363),\n",
      " (13, 0.09251498683665028),\n",
      " (14, 0.08179147692528434),\n",
      " (15, 0.07644894993957736),\n",
      " (16, 0.1100888385863401),\n",
      " (17, 0.10662800080529497),\n",
      " (18, 0.08069328857353127),\n",
      " (19, 0.0887079631548694),\n",
      " (20, 0.2091478825837195),\n",
      " (21, 0.1141456719262901),\n",
      " (22, 0.06995842651363943),\n",
      " (23, 0.3137218238755793),\n",
      " (24, 0.10010654469215592),\n",
      " (25, 0.07444474639484122),\n",
      " (26, 0.21545237831409603),\n",
      " (27, 0.09080772799217456),\n",
      " (28, 0.08677980683269663),\n",
      " (29, 0.11271451906446033),\n",
      " (30, 0.05648969248226538),\n",
      " (31, 0.1100888385863401),\n",
      " (32, 0.06277325092310569),\n",
      " (33, 0.0677911447572431),\n",
      " (34, 0.11271451906446033),\n",
      " (35, 0.04761232932971362),\n",
      " (36, 0.09930623779955326),\n",
      " (37, 0.17449456668868762),\n",
      " (38, 0.07767555270991344),\n",
      " (39, 0.09635153746712699),\n",
      " (40, 0.05428642475486903),\n",
      " (41, 0.07896939937091467),\n",
      " (42, 0.12866061395239586),\n",
      " (43, 0.05623448143008929),\n",
      " (44, 0.1113651922547627),\n",
      " (45, 0.06099989379434012),\n",
      " (46, 0.054878104659937726),\n",
      " (47, 0.04124323461034833),\n",
      " (48, 0.09435540876167076),\n",
      " (49, 0.26246639301881597),\n",
      " (50, 0.2072205298833543),\n",
      " (51, 0.052704516828332375),\n",
      " (52, 0.11146438060406727),\n",
      " (53, 0.14888949278968244),\n",
      " (54, 0.1141456719262901),\n",
      " (55, 0.06677782213764862),\n",
      " (56, 0.08105370461866626),\n",
      " (57, 0.09930623779955326),\n",
      " (58, 0.11566921939688658),\n",
      " (59, 0.050714296467896235),\n",
      " (60, 0.06928709807811127),\n",
      " (61, 0.11566921939688658),\n",
      " (62, 0.0704168252353633),\n",
      " (63, 0.05325739437962246),\n",
      " (64, 0.07799242716433741),\n",
      " (65, 0.05935049476413499),\n",
      " (66, 0.051324850993247406),\n",
      " (67, 0.07585874938856867),\n",
      " (68, 0.04599537653048972),\n",
      " (69, 0.09311265907387573),\n",
      " (70, 0.0697681102966082),\n",
      " (71, 0.10787681354065659),\n",
      " (72, 0.32663390530195086),\n",
      " (73, 0.08724728334434381),\n",
      " (74, 0.04641026932577236),\n",
      " (75, 0.16138657714706253),\n",
      " (76, 0.06738058149748925),\n",
      " (77, 0.10093493819414506),\n",
      " (78, 0.028586607917913454),\n",
      " (79, 0.04019094709437606)]\n",
      "Topic: 0 \n",
      "Words: 0.015*\"time\" + 0.013*\"stori\" + 0.010*\"life\" + 0.009*\"book\" + 0.007*\"love\" + 0.006*\"live\" + 0.006*\"person\" + 0.005*\"world\" + 0.005*\"help\" + 0.005*\"work\"\n",
      "Topic: 1 \n",
      "Words: 0.032*\"book\" + 0.009*\"time\" + 0.009*\"read\" + 0.008*\"stori\" + 0.008*\"world\" + 0.007*\"life\" + 0.006*\"illustr\" + 0.006*\"best\" + 0.006*\"color\" + 0.005*\"children\"\n",
      "Topic: 2 \n",
      "Words: 0.009*\"work\" + 0.008*\"manag\" + 0.008*\"busi\" + 0.007*\"book\" + 0.006*\"market\" + 0.006*\"time\" + 0.005*\"help\" + 0.005*\"know\" + 0.005*\"life\" + 0.005*\"world\"\n",
      "Topic: 3 \n",
      "Words: 0.011*\"book\" + 0.010*\"world\" + 0.008*\"love\" + 0.007*\"time\" + 0.007*\"life\" + 0.006*\"peopl\" + 0.005*\"year\" + 0.005*\"know\" + 0.005*\"histori\" + 0.004*\"work\"\n",
      "Topic: 4 \n",
      "Words: 0.015*\"book\" + 0.010*\"year\" + 0.009*\"know\" + 0.007*\"life\" + 0.006*\"world\" + 0.006*\"work\" + 0.005*\"like\" + 0.005*\"want\" + 0.005*\"polit\" + 0.004*\"time\"\n",
      "Topic: 5 \n",
      "Words: 0.011*\"book\" + 0.009*\"life\" + 0.009*\"famili\" + 0.008*\"time\" + 0.008*\"year\" + 0.007*\"world\" + 0.007*\"love\" + 0.006*\"best\" + 0.006*\"live\" + 0.006*\"help\"\n",
      "Topic: 6 \n",
      "Words: 0.011*\"book\" + 0.008*\"year\" + 0.008*\"life\" + 0.006*\"time\" + 0.006*\"american\" + 0.006*\"author\" + 0.006*\"stori\" + 0.005*\"live\" + 0.005*\"work\" + 0.005*\"world\"\n",
      "Topic: 7 \n",
      "Words: 0.013*\"book\" + 0.010*\"life\" + 0.010*\"time\" + 0.009*\"stori\" + 0.006*\"author\" + 0.006*\"world\" + 0.006*\"nation\" + 0.005*\"york\" + 0.005*\"year\" + 0.005*\"includ\"\n",
      "Topic: 8 \n",
      "Words: 0.013*\"life\" + 0.010*\"live\" + 0.009*\"world\" + 0.009*\"time\" + 0.009*\"book\" + 0.007*\"work\" + 0.007*\"stori\" + 0.007*\"year\" + 0.006*\"like\" + 0.006*\"best\"\n",
      "Topic: 9 \n",
      "Words: 0.017*\"book\" + 0.011*\"world\" + 0.007*\"life\" + 0.007*\"stori\" + 0.007*\"time\" + 0.006*\"learn\" + 0.005*\"includ\" + 0.005*\"reader\" + 0.004*\"busi\" + 0.004*\"power\"\n",
      "Topic: 0 Word: 0.003*\"life\" + 0.003*\"love\" + 0.003*\"book\" + 0.003*\"famili\" + 0.003*\"color\" + 0.003*\"stori\" + 0.003*\"world\" + 0.003*\"time\" + 0.002*\"peopl\" + 0.002*\"leav\"\n",
      "Topic: 1 Word: 0.004*\"recip\" + 0.003*\"life\" + 0.002*\"time\" + 0.002*\"book\" + 0.002*\"famili\" + 0.002*\"year\" + 0.002*\"live\" + 0.002*\"reader\" + 0.002*\"word\" + 0.002*\"food\"\n",
      "Topic: 2 Word: 0.004*\"church\" + 0.003*\"care\" + 0.003*\"famili\" + 0.003*\"book\" + 0.003*\"love\" + 0.003*\"world\" + 0.002*\"life\" + 0.002*\"time\" + 0.002*\"novel\" + 0.002*\"check\"\n",
      "Topic: 3 Word: 0.004*\"book\" + 0.003*\"stori\" + 0.003*\"learn\" + 0.002*\"live\" + 0.002*\"power\" + 0.002*\"life\" + 0.002*\"time\" + 0.002*\"school\" + 0.002*\"help\" + 0.002*\"illustr\"\n",
      "Topic: 4 Word: 0.003*\"monster\" + 0.003*\"human\" + 0.003*\"love\" + 0.003*\"book\" + 0.003*\"world\" + 0.002*\"reader\" + 0.002*\"camera\" + 0.002*\"life\" + 0.002*\"follow\" + 0.002*\"friend\"\n",
      "Topic: 5 Word: 0.003*\"book\" + 0.003*\"brand\" + 0.003*\"life\" + 0.003*\"love\" + 0.003*\"know\" + 0.002*\"time\" + 0.002*\"world\" + 0.002*\"busi\" + 0.002*\"inspir\" + 0.002*\"thing\"\n",
      "Topic: 6 Word: 0.004*\"book\" + 0.003*\"life\" + 0.003*\"world\" + 0.003*\"year\" + 0.003*\"dead\" + 0.003*\"mother\" + 0.002*\"live\" + 0.002*\"stori\" + 0.002*\"love\" + 0.002*\"black\"\n",
      "Topic: 7 Word: 0.003*\"book\" + 0.003*\"stori\" + 0.003*\"love\" + 0.003*\"work\" + 0.003*\"life\" + 0.003*\"music\" + 0.003*\"busi\" + 0.002*\"time\" + 0.002*\"girl\" + 0.002*\"park\"\n",
      "Topic: 8 Word: 0.003*\"centuri\" + 0.003*\"wild\" + 0.003*\"book\" + 0.003*\"collect\" + 0.003*\"cook\" + 0.003*\"food\" + 0.003*\"bird\" + 0.003*\"world\" + 0.003*\"photograph\" + 0.002*\"cultur\"\n",
      "Topic: 9 Word: 0.003*\"book\" + 0.003*\"sale\" + 0.003*\"time\" + 0.003*\"world\" + 0.003*\"busi\" + 0.002*\"life\" + 0.002*\"manag\" + 0.002*\"children\" + 0.002*\"author\" + 0.002*\"bestsel\"\n",
      "['christma', 'jolli', 'postman', 'deliv', 'greet', 'fairi', 'tale', 'charact', 'card', 'babi', 'bear', 'game', 'appropri', 'call', 'bewar', 'rid', 'hood', 'wolf', 'jigsaw', 'hospitalis', 'humpti', 'dumpti', 'surpris', 'envelop', 'contain', 'letter', 'card', 'favourit', 'postman', 'keep', 'peddl', 'bicycl', 'hill', 'dale', 'everybodi', 'heart']\n",
      "\n",
      "Score: 0.8691292405128479\t \n",
      "Topic: 0.013*\"life\" + 0.010*\"live\" + 0.009*\"world\" + 0.009*\"time\" + 0.009*\"book\" + 0.007*\"work\" + 0.007*\"stori\" + 0.007*\"year\" + 0.006*\"like\" + 0.006*\"best\"\n",
      "\n",
      "Score: 0.11683253943920135\t \n",
      "Topic: 0.009*\"work\" + 0.008*\"manag\" + 0.008*\"busi\" + 0.007*\"book\" + 0.006*\"market\" + 0.006*\"time\" + 0.005*\"help\" + 0.005*\"know\" + 0.005*\"life\" + 0.005*\"world\"\n",
      "\n",
      "Score: 0.9842069745063782\t \n",
      "Topic: 0.003*\"book\" + 0.003*\"sale\" + 0.003*\"time\" + 0.003*\"world\" + 0.003*\"busi\" + 0.002*\"life\" + 0.002*\"manag\" + 0.002*\"children\" + 0.002*\"author\" + 0.002*\"bestsel\"\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e85c9f887db0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtopic_categories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-2ec7237560c1>\u001b[0m in \u001b[0;36mtopic_categories\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mdoc_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m55\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'original document: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "for i in [1,2,3,4,5]:\n",
    "    df=df[df['Category']==i]\n",
    "    topic_categories(df)\n",
    "\n",
    "#loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sanjayshah/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "def topic_categories(df):\n",
    "    \n",
    "\n",
    "    data_text = df[['Description']]\n",
    "    data_text['index'] = data_text.index\n",
    "    documents = data_text\n",
    "\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    def lemmatize_stemming(text):\n",
    "        return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "    def preprocess(text):\n",
    "        result = []\n",
    "        for token in gensim.utils.simple_preprocess(text):\n",
    "            if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "                result.append(lemmatize_stemming(token))\n",
    "        return result\n",
    "\n",
    "    doc_sample = documents[documents['index'] == 55].values[0][0]\n",
    "    print('original document: ')\n",
    "    words = []\n",
    "    for word in doc_sample.split(' '):\n",
    "        words.append(word)\n",
    "    print(words)\n",
    "    print('\\n\\n tokenized and lemmatized document: ')\n",
    "    print(preprocess(doc_sample))\n",
    "    processed_docs = documents['Description'].map(preprocess)\n",
    "    dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "    count = 0\n",
    "    for k, v in dictionary.iteritems():\n",
    "        print(k, v)\n",
    "        count += 1\n",
    "        if count > 10:\n",
    "            break\n",
    "    dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "    print(bow_corpus[1])\n",
    "    bow_doc_1 = bow_corpus[1]\n",
    "\n",
    "    for i in range(len(bow_doc_1)):\n",
    "        print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_1[i][0], \n",
    "                                                         dictionary[bow_doc_1[i][0]], \n",
    "                                                         bow_doc_1[i][1]))\n",
    "    from gensim import corpora, models\n",
    "\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "    corpus_tfidf = tfidf[bow_corpus]\n",
    "    from pprint import pprint\n",
    "\n",
    "    for doc in corpus_tfidf:\n",
    "        pprint(doc)\n",
    "        break\n",
    "    lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "    lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "    for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "        print('Topic: {} Word: {}'.format(idx, topic))\n",
    "    print(processed_docs[55])\n",
    "    for index, score in sorted(lda_model[bow_corpus[1]], key=lambda tup: -1*tup[1]):\n",
    "        print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))\n",
    "    for index, score in sorted(lda_model_tfidf[bow_corpus[1]], key=lambda tup: -1*tup[1]):\n",
    "        print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
