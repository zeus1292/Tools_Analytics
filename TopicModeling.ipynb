{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjayshah/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('df_processed.csv',error_bad_lines=False);\n",
    "data = data.dropna(how='any',axis=0)\n",
    "data_text = data[['Description']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Instant New York Times and USA Today Best...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phaedra Patrick's debut novel, The Curious Ch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Whether it's selling your company's product i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Christian Apologetics: An Anthology of Primar...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>An instant New York Times Bestseller and Augu...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>We’ve heard plenty from politicians and exper...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>After serving out a year of hard labor in the...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Donald Trump won the presidency by being a on...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Grad student and cat lover Dulcie Schwartz m...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Cook, eat, and be fit with 200 recipes from B...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Description  index\n",
       "0    The Instant New York Times and USA Today Best...      0\n",
       "1    Phaedra Patrick's debut novel, The Curious Ch...      1\n",
       "2    Whether it's selling your company's product i...      2\n",
       "3    Christian Apologetics: An Anthology of Primar...      3\n",
       "5    An instant New York Times Bestseller and Augu...      5\n",
       "8    We’ve heard plenty from politicians and exper...      8\n",
       "10   After serving out a year of hard labor in the...     10\n",
       "14   Donald Trump won the presidency by being a on...     14\n",
       "16    Grad student and cat lover Dulcie Schwartz m...     16\n",
       "20   Cook, eat, and be fit with 200 recipes from B...     20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The Instant New York Times and USA Today Bestseller!When Washington D.C. was first built, it was on top of a swamp that had to be drained. Donald Trump says it\\'s time to drain it again. In The Swamp, bestselling author Eric Bolling presents an infuriating, amusing, revealing, and outrageous history of American politics, past and present, Republican and Democrat. From national political scandals to tempests in a teapot that blew up; bribery, blackmail, bullying, and backroom deals that contradicted public policies; cronyism that cost taxpayers hundreds upon hundreds of millions of dollars; and personal conduct that can only be described as regrettable, The Swamp is a journey downriver through the bayous and marshes of Capitol Hill and Foggy Bottom. The presidential election of 2016 was ugly, but it exposed a political, media, industry, and elite establishment that desperately wanted to elect a politician who received millions of dollars from terror-funding states over a businessman willing to tell the corrupt or incompetent, “You’re fired.”The book concludes with a series of recommendations for President Trump: practical, hard-headed, and concise ways to drain the swamp and force Washington to be more transparent, more accountable, and more effective in how it serves those who have elected its politicians and pay the bills for their decisions.Last year President Trump declared Wake Up America to be a \"huge\" book; Eric Bolling\\'s second book is sure to build on that success. Entertaining and timely, The Swamp is the perfect book for today\\'s political climate.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['Description'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sanjayshah/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['', 'Part', 'road-trip', 'tale,', 'part', 'travelogue', 'of', 'lost', 'and', 'found', 'landscapes,', 'all', 'good-natured', 'natural', 'history,', 'Mariposa', 'Road', 'tracks', 'Bob', 'Pyle’s', 'journey', 'across', 'the', 'United', 'States', 'as', 'he', 'races', 'against', 'the', 'calendarin', 'his', 'search', 'for', 'as', 'many', 'of', 'the', '800', 'American', 'butterflies', 'as', 'he', 'can', 'find.\\xa0Like', 'Pyle’s', 'classic', 'Chasing', 'Monarchs,', 'Mariposa', 'Road', 'recounts', 'his', 'adventures,', 'high', 'and', 'low,', 'in', 'tracking', 'down', 'butterflies', 'in', 'his', 'own', 'low-tech,', 'individual', 'way.', 'Accompanied', 'by', 'Marsha,', 'his', 'cottonwood-limb', 'butterfly', 'net;', 'Powdermilk,', 'his', '1982', 'Honda', 'Civic', 'with', '345,000', 'miles', 'on', 'the', 'odometer;', 'and', 'the', 'small', 'Leitz', 'binoculars', 'he', 'has', 'carried', 'for', 'more', 'than', 'thirty', 'years,', 'Bob', 'ventured', 'out', 'in', 'a', 'series', 'of', 'remarkable', 'trips', 'from', 'his', 'Northwest', 'home.\\xa0From', 'the', 'California', 'coastline', 'in', 'company', 'with', 'overwintering', 'monarchs', 'to', 'the', 'Far', 'Northern', 'tundra', 'in', 'pursuit', 'of', 'mysterious', 'sulphurs', 'and', 'arctics;', 'from', 'the', 'zebras', 'and', 'daggerwings', 'of', 'the', 'Everglades', 'to', 'the', 'leafwings,', 'bluewings,', 'and', 'border', 'rarities', 'of', 'the', 'lower', 'Rio', 'Grande;', 'from', 'Graceland', 'to', 'ranchland', 'and', 'Kauai', 'to', 'Key', 'West,', 'these', 'intimate', 'encounters', 'with', 'the', 'land,', 'its', 'people,\\xa0and', 'its', 'fading', 'fauna', 'are', 'wholly', 'original.', 'At', 'turns', 'whimsical,', 'witty,', 'informative,', 'and', 'inspirational,', 'Mariposa', 'Road\\xa0is', 'an\\xa0extraordinary', 'journey', 'of', 'discovery', 'that\\xa0leads', 'the', 'reader', 'ever', 'farther', 'into', 'butterfly', 'country', 'and\\xa0deeper', 'into', 'the', 'heart', 'of', 'the', 'naturalist.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['road', 'trip', 'tale', 'travelogu', 'lose', 'landscap', 'good', 'natur', 'natur', 'histori', 'mariposa', 'road', 'track', 'pyle', 'journey', 'unit', 'state', 'race', 'calendarin', 'search', 'american', 'butterfli', 'like', 'pyle', 'classic', 'chase', 'monarch', 'mariposa', 'road', 'recount', 'adventur', 'high', 'track', 'butterfli', 'tech', 'individu', 'accompani', 'marsha', 'cottonwood', 'limb', 'butterfli', 'powdermilk', 'honda', 'civic', 'mile', 'odomet', 'small', 'leitz', 'binocular', 'carri', 'thirti', 'year', 'ventur', 'seri', 'remark', 'trip', 'northwest', 'home', 'california', 'coastlin', 'compani', 'overwint', 'monarch', 'northern', 'tundra', 'pursuit', 'mysteri', 'sulphur', 'arctic', 'zebra', 'daggerw', 'everglad', 'leafw', 'bluew', 'border', 'rariti', 'lower', 'grand', 'graceland', 'ranchland', 'kauai', 'west', 'intim', 'encount', 'land', 'peopl', 'fade', 'fauna', 'wholli', 'origin', 'turn', 'whimsic', 'witti', 'inform', 'inspir', 'mariposa', 'road', 'extraordinari', 'journey', 'discoveri', 'lead', 'reader', 'farther', 'butterfli', 'countri', 'deeper', 'heart', 'naturalist']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 1].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['Description'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [instant, york, time, today, bestsel, washingt...\n",
       "1     [phaedra, patrick, debut, novel, curious, char...\n",
       "2     [sell, compani, product, boardroom, sell, eat,...\n",
       "3     [christian, apologet, antholog, primari, sourc...\n",
       "5     [instant, york, time, bestsel, august, library...\n",
       "8     [hear, plenti, politician, expert, affirm, act...\n",
       "10    [serv, year, hard, labor, salt, mine, endovi, ...\n",
       "14    [donald, trump, presid, wreck, ball, dysfunct,...\n",
       "16    [grad, student, lover, dulci, schwartz, fight,...\n",
       "20    [cook, recip, bobbi, flay, approach, healthi, ...\n",
       "Name: Description, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['Description'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 account\n",
      "1 america\n",
      "2 american\n",
      "3 amus\n",
      "4 author\n",
      "5 backroom\n",
      "6 bayous\n",
      "7 bestsel\n",
      "8 bill\n",
      "9 blackmail\n",
      "10 blow\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to ‘bow_corpus’, then check our selected document earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 2),\n",
       " (6, 1),\n",
       " (7, 4),\n",
       " (8, 2),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (15, 1),\n",
       " (16, 1),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 1),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 2),\n",
       " (24, 1),\n",
       " (25, 1),\n",
       " (26, 3),\n",
       " (27, 1),\n",
       " (28, 1),\n",
       " (29, 2),\n",
       " (30, 1),\n",
       " (31, 1),\n",
       " (32, 1),\n",
       " (33, 1),\n",
       " (34, 1),\n",
       " (35, 1),\n",
       " (36, 1),\n",
       " (37, 1),\n",
       " (38, 1),\n",
       " (39, 1),\n",
       " (40, 2),\n",
       " (41, 1),\n",
       " (42, 1),\n",
       " (43, 1),\n",
       " (44, 1),\n",
       " (45, 2),\n",
       " (46, 1),\n",
       " (47, 1),\n",
       " (48, 1),\n",
       " (49, 1),\n",
       " (50, 1),\n",
       " (51, 1),\n",
       " (52, 4),\n",
       " (53, 2),\n",
       " (54, 1),\n",
       " (55, 2),\n",
       " (56, 2),\n",
       " (57, 1),\n",
       " (58, 1),\n",
       " (59, 1),\n",
       " (60, 1),\n",
       " (61, 1),\n",
       " (62, 1),\n",
       " (63, 1),\n",
       " (64, 1),\n",
       " (65, 1),\n",
       " (66, 1),\n",
       " (67, 1),\n",
       " (68, 1),\n",
       " (69, 1),\n",
       " (70, 1),\n",
       " (71, 1),\n",
       " (72, 1),\n",
       " (73, 3),\n",
       " (74, 2),\n",
       " (75, 3),\n",
       " (76, 1),\n",
       " (77, 1),\n",
       " (78, 2),\n",
       " (79, 1),\n",
       " (80, 1),\n",
       " (81, 1),\n",
       " (82, 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"account\") appears 1 time.\n",
      "Word 1 (\"america\") appears 1 time.\n",
      "Word 2 (\"american\") appears 1 time.\n",
      "Word 3 (\"amus\") appears 1 time.\n",
      "Word 4 (\"author\") appears 1 time.\n",
      "Word 5 (\"bestsel\") appears 2 time.\n",
      "Word 6 (\"blow\") appears 1 time.\n",
      "Word 7 (\"book\") appears 4 time.\n",
      "Word 8 (\"build\") appears 2 time.\n",
      "Word 9 (\"bulli\") appears 1 time.\n",
      "Word 10 (\"climat\") appears 1 time.\n",
      "Word 11 (\"concis\") appears 1 time.\n",
      "Word 12 (\"conclud\") appears 1 time.\n",
      "Word 13 (\"conduct\") appears 1 time.\n",
      "Word 14 (\"contradict\") appears 1 time.\n",
      "Word 15 (\"corrupt\") appears 1 time.\n",
      "Word 16 (\"cost\") appears 1 time.\n",
      "Word 17 (\"deal\") appears 1 time.\n",
      "Word 18 (\"decis\") appears 1 time.\n",
      "Word 19 (\"declar\") appears 1 time.\n",
      "Word 20 (\"democrat\") appears 1 time.\n",
      "Word 21 (\"describ\") appears 1 time.\n",
      "Word 22 (\"desper\") appears 1 time.\n",
      "Word 23 (\"dollar\") appears 2 time.\n",
      "Word 24 (\"donald\") appears 1 time.\n",
      "Word 25 (\"effect\") appears 1 time.\n",
      "Word 26 (\"elect\") appears 3 time.\n",
      "Word 27 (\"elit\") appears 1 time.\n",
      "Word 28 (\"entertain\") appears 1 time.\n",
      "Word 29 (\"eric\") appears 2 time.\n",
      "Word 30 (\"establish\") appears 1 time.\n",
      "Word 31 (\"expos\") appears 1 time.\n",
      "Word 32 (\"fire\") appears 1 time.\n",
      "Word 33 (\"forc\") appears 1 time.\n",
      "Word 34 (\"fund\") appears 1 time.\n",
      "Word 35 (\"hard\") appears 1 time.\n",
      "Word 36 (\"head\") appears 1 time.\n",
      "Word 37 (\"hill\") appears 1 time.\n",
      "Word 38 (\"histori\") appears 1 time.\n",
      "Word 39 (\"huge\") appears 1 time.\n",
      "Word 40 (\"hundr\") appears 2 time.\n",
      "Word 41 (\"industri\") appears 1 time.\n",
      "Word 42 (\"instant\") appears 1 time.\n",
      "Word 43 (\"journey\") appears 1 time.\n",
      "Word 44 (\"media\") appears 1 time.\n",
      "Word 45 (\"million\") appears 2 time.\n",
      "Word 46 (\"nation\") appears 1 time.\n",
      "Word 47 (\"outrag\") appears 1 time.\n",
      "Word 48 (\"past\") appears 1 time.\n",
      "Word 49 (\"perfect\") appears 1 time.\n",
      "Word 50 (\"person\") appears 1 time.\n",
      "Word 51 (\"polici\") appears 1 time.\n",
      "Word 52 (\"polit\") appears 4 time.\n",
      "Word 53 (\"politician\") appears 2 time.\n",
      "Word 54 (\"practic\") appears 1 time.\n",
      "Word 55 (\"present\") appears 2 time.\n",
      "Word 56 (\"presid\") appears 2 time.\n",
      "Word 57 (\"presidenti\") appears 1 time.\n",
      "Word 58 (\"public\") appears 1 time.\n",
      "Word 59 (\"receiv\") appears 1 time.\n",
      "Word 60 (\"recommend\") appears 1 time.\n",
      "Word 61 (\"republican\") appears 1 time.\n",
      "Word 62 (\"reveal\") appears 1 time.\n",
      "Word 63 (\"say\") appears 1 time.\n",
      "Word 64 (\"scandal\") appears 1 time.\n",
      "Word 65 (\"second\") appears 1 time.\n",
      "Word 66 (\"seri\") appears 1 time.\n",
      "Word 67 (\"serv\") appears 1 time.\n",
      "Word 68 (\"state\") appears 1 time.\n",
      "Word 69 (\"success\") appears 1 time.\n",
      "Word 70 (\"sure\") appears 1 time.\n",
      "Word 71 (\"tell\") appears 1 time.\n",
      "Word 72 (\"terror\") appears 1 time.\n",
      "Word 73 (\"time\") appears 3 time.\n",
      "Word 74 (\"today\") appears 2 time.\n",
      "Word 75 (\"trump\") appears 3 time.\n",
      "Word 76 (\"wake\") appears 1 time.\n",
      "Word 77 (\"want\") appears 1 time.\n",
      "Word 78 (\"washington\") appears 2 time.\n",
      "Word 79 (\"way\") appears 1 time.\n",
      "Word 80 (\"will\") appears 1 time.\n",
      "Word 81 (\"year\") appears 1 time.\n",
      "Word 82 (\"york\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_0 = bow_corpus[0]\n",
    "\n",
    "for i in range(len(bow_doc_0)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_0[i][0], \n",
    "                                                     dictionary[bow_doc_0[i][0]], \n",
    "                                                     bow_doc_0[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.1896817565175649),\n",
      " (1, 0.13616465623012936),\n",
      " (2, 0.1815469917216657),\n",
      " (3, 0.1777799590017366),\n",
      " (4, 0.19877564986109572),\n",
      " (5, 0.1777799590017366),\n",
      " (6, 0.1777799590017366),\n",
      " (7, 0.17075613215210664),\n",
      " (8, 0.14770983022422038),\n",
      " (9, 0.15837625970208172),\n",
      " (10, 0.13616465623012936),\n",
      " (11, 0.3793635130351298),\n",
      " (12, 0.1896817565175649),\n",
      " (13, 0.11893599809069934),\n",
      " (14, 0.1777799590017366),\n",
      " (15, 0.1940948922144773),\n",
      " (16, 0.3793635130351298),\n",
      " (17, 0.16747015304561255),\n",
      " (18, 0.13616465623012936),\n",
      " (19, 0.13616465623012936),\n",
      " (20, 0.20375859519361803),\n",
      " (21, 0.18550728826012086),\n",
      " (22, 0.1896817565175649),\n",
      " (23, 0.1815469917216657),\n",
      " (24, 0.1815469917216657),\n",
      " (25, 0.16129014722003585),\n",
      " (26, 0.08593353504918798)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.015*\"book\" + 0.010*\"life\" + 0.008*\"time\" + 0.008*\"love\" + 0.007*\"famili\" + 0.006*\"world\" + 0.006*\"stori\" + 0.006*\"year\" + 0.005*\"live\" + 0.005*\"best\"\n",
      "Topic: 1 \n",
      "Words: 0.010*\"book\" + 0.009*\"world\" + 0.008*\"time\" + 0.008*\"life\" + 0.005*\"author\" + 0.005*\"live\" + 0.005*\"peopl\" + 0.005*\"learn\" + 0.005*\"help\" + 0.004*\"love\"\n",
      "Topic: 2 \n",
      "Words: 0.016*\"book\" + 0.006*\"work\" + 0.006*\"time\" + 0.006*\"world\" + 0.006*\"year\" + 0.006*\"life\" + 0.006*\"stori\" + 0.005*\"busi\" + 0.005*\"chang\" + 0.004*\"american\"\n",
      "Topic: 3 \n",
      "Words: 0.011*\"world\" + 0.009*\"life\" + 0.009*\"time\" + 0.006*\"peopl\" + 0.006*\"know\" + 0.005*\"year\" + 0.005*\"help\" + 0.005*\"like\" + 0.005*\"stori\" + 0.004*\"work\"\n",
      "Topic: 4 \n",
      "Words: 0.013*\"book\" + 0.008*\"stori\" + 0.007*\"time\" + 0.006*\"author\" + 0.005*\"year\" + 0.004*\"read\" + 0.004*\"world\" + 0.004*\"live\" + 0.004*\"star\" + 0.004*\"american\"\n",
      "Topic: 5 \n",
      "Words: 0.008*\"life\" + 0.008*\"stori\" + 0.008*\"time\" + 0.008*\"know\" + 0.007*\"year\" + 0.006*\"love\" + 0.006*\"come\" + 0.005*\"like\" + 0.005*\"friend\" + 0.005*\"world\"\n",
      "Topic: 6 \n",
      "Words: 0.014*\"book\" + 0.008*\"work\" + 0.006*\"histori\" + 0.006*\"world\" + 0.005*\"life\" + 0.005*\"american\" + 0.005*\"stori\" + 0.005*\"includ\" + 0.005*\"design\" + 0.004*\"year\"\n",
      "Topic: 7 \n",
      "Words: 0.012*\"book\" + 0.010*\"time\" + 0.008*\"world\" + 0.008*\"stori\" + 0.007*\"life\" + 0.006*\"year\" + 0.005*\"best\" + 0.005*\"live\" + 0.005*\"author\" + 0.005*\"york\"\n",
      "Topic: 8 \n",
      "Words: 0.012*\"book\" + 0.009*\"time\" + 0.006*\"stori\" + 0.005*\"author\" + 0.005*\"learn\" + 0.005*\"power\" + 0.005*\"year\" + 0.005*\"york\" + 0.005*\"life\" + 0.005*\"live\"\n",
      "Topic: 9 \n",
      "Words: 0.013*\"book\" + 0.008*\"help\" + 0.007*\"time\" + 0.005*\"world\" + 0.005*\"food\" + 0.005*\"like\" + 0.004*\"stori\" + 0.004*\"year\" + 0.004*\"recip\" + 0.004*\"school\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.003*\"book\" + 0.003*\"life\" + 0.003*\"color\" + 0.002*\"think\" + 0.002*\"love\" + 0.002*\"live\" + 0.002*\"time\" + 0.002*\"stori\" + 0.002*\"world\" + 0.002*\"help\"\n",
      "Topic: 1 Word: 0.006*\"book\" + 0.002*\"color\" + 0.002*\"life\" + 0.002*\"health\" + 0.002*\"illustr\" + 0.002*\"help\" + 0.002*\"famili\" + 0.002*\"world\" + 0.002*\"design\" + 0.002*\"chang\"\n",
      "Topic: 2 Word: 0.003*\"book\" + 0.002*\"stori\" + 0.002*\"world\" + 0.002*\"life\" + 0.002*\"like\" + 0.002*\"live\" + 0.002*\"time\" + 0.002*\"know\" + 0.002*\"histori\" + 0.002*\"american\"\n",
      "Topic: 3 Word: 0.002*\"book\" + 0.002*\"world\" + 0.002*\"stori\" + 0.002*\"life\" + 0.002*\"love\" + 0.002*\"polit\" + 0.002*\"live\" + 0.002*\"year\" + 0.002*\"time\" + 0.002*\"work\"\n",
      "Topic: 4 Word: 0.003*\"book\" + 0.002*\"love\" + 0.002*\"children\" + 0.002*\"stori\" + 0.002*\"life\" + 0.002*\"mother\" + 0.002*\"heart\" + 0.002*\"author\" + 0.002*\"world\" + 0.002*\"famili\"\n",
      "Topic: 5 Word: 0.002*\"book\" + 0.002*\"game\" + 0.002*\"life\" + 0.002*\"stori\" + 0.002*\"live\" + 0.002*\"time\" + 0.002*\"world\" + 0.002*\"famili\" + 0.002*\"love\" + 0.002*\"person\"\n",
      "Topic: 6 Word: 0.002*\"american\" + 0.002*\"love\" + 0.002*\"life\" + 0.002*\"famili\" + 0.002*\"book\" + 0.002*\"time\" + 0.002*\"york\" + 0.002*\"stori\" + 0.002*\"trump\" + 0.002*\"world\"\n",
      "Topic: 7 Word: 0.002*\"love\" + 0.002*\"world\" + 0.002*\"book\" + 0.002*\"church\" + 0.002*\"life\" + 0.002*\"jesus\" + 0.002*\"know\" + 0.002*\"famili\" + 0.002*\"year\" + 0.002*\"peopl\"\n",
      "Topic: 8 Word: 0.003*\"stori\" + 0.002*\"world\" + 0.002*\"book\" + 0.002*\"life\" + 0.002*\"time\" + 0.002*\"busi\" + 0.002*\"year\" + 0.002*\"best\" + 0.002*\"leav\" + 0.002*\"love\"\n",
      "Topic: 9 Word: 0.003*\"book\" + 0.002*\"love\" + 0.002*\"life\" + 0.002*\"time\" + 0.002*\"stori\" + 0.002*\"work\" + 0.002*\"world\" + 0.002*\"york\" + 0.002*\"live\" + 0.002*\"peopl\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['phaedra',\n",
       " 'patrick',\n",
       " 'debut',\n",
       " 'novel',\n",
       " 'curious',\n",
       " 'charm',\n",
       " 'arthur',\n",
       " 'pepper',\n",
       " 'hail',\n",
       " 'poignant',\n",
       " 'utter',\n",
       " 'endear',\n",
       " 'return',\n",
       " 'rise',\n",
       " 'shine',\n",
       " 'benedict',\n",
       " 'stone',\n",
       " 'novel',\n",
       " 'famili',\n",
       " 'forgiv',\n",
       " 'second',\n",
       " 'chanc',\n",
       " 'happi',\n",
       " 'moonston',\n",
       " 'empathi',\n",
       " 'azurit',\n",
       " 'memori',\n",
       " 'lapi',\n",
       " 'lazuli',\n",
       " 'truth',\n",
       " 'quiet',\n",
       " 'villag',\n",
       " 'noon',\n",
       " 'benedict',\n",
       " 'stone',\n",
       " 'settl',\n",
       " 'complac',\n",
       " 'predict',\n",
       " 'routin',\n",
       " 'busi',\n",
       " 'jewelri',\n",
       " 'shop',\n",
       " 'dri',\n",
       " 'marriag',\n",
       " 'rock',\n",
       " 'life',\n",
       " 'desper',\n",
       " 'need',\n",
       " 'jump',\n",
       " 'start',\n",
       " 'surpris',\n",
       " 'arriv',\n",
       " 'door',\n",
       " 'gemma',\n",
       " 'benedict',\n",
       " 'audaci',\n",
       " 'teenag',\n",
       " 'niec',\n",
       " 'daughter',\n",
       " 'estrang',\n",
       " 'brother',\n",
       " 'charli',\n",
       " 'stone',\n",
       " 'brother',\n",
       " 'fall',\n",
       " 'haven',\n",
       " 'speak',\n",
       " 'decad',\n",
       " 'charli',\n",
       " 'leav',\n",
       " 'america',\n",
       " 'reckless',\n",
       " 'stubborn',\n",
       " 'gemma',\n",
       " 'invit',\n",
       " 'benedict',\n",
       " 'world',\n",
       " 'turn',\n",
       " 'order',\n",
       " 'life',\n",
       " 'upsid',\n",
       " 'exact',\n",
       " 'need',\n",
       " 'life',\n",
       " 'track',\n",
       " 'fill',\n",
       " 'color',\n",
       " 'charact',\n",
       " 'irresist',\n",
       " 'charm',\n",
       " 'rise',\n",
       " 'shine',\n",
       " 'benedict',\n",
       " 'stone',\n",
       " 'lumin',\n",
       " 'remind',\n",
       " 'unbreak',\n",
       " 'bond',\n",
       " 'famili',\n",
       " 'show',\n",
       " 'have',\n",
       " 'embrac',\n",
       " 'life',\n",
       " 'better',\n",
       " 'stand']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.7912186980247498\t \n",
      "Topic: 0.008*\"life\" + 0.008*\"stori\" + 0.008*\"time\" + 0.008*\"know\" + 0.007*\"year\" + 0.006*\"love\" + 0.006*\"come\" + 0.005*\"like\" + 0.005*\"friend\" + 0.005*\"world\"\n",
      "\n",
      "Score: 0.19890256226062775\t \n",
      "Topic: 0.015*\"book\" + 0.010*\"life\" + 0.008*\"time\" + 0.008*\"love\" + 0.007*\"famili\" + 0.006*\"world\" + 0.006*\"stori\" + 0.006*\"year\" + 0.005*\"live\" + 0.005*\"best\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[1]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.3969144821166992\t \n",
      "Topic: 0.003*\"stori\" + 0.002*\"world\" + 0.002*\"book\" + 0.002*\"life\" + 0.002*\"time\" + 0.002*\"busi\" + 0.002*\"year\" + 0.002*\"best\" + 0.002*\"leav\" + 0.002*\"love\"\n",
      "\n",
      "Score: 0.3961485028266907\t \n",
      "Topic: 0.002*\"book\" + 0.002*\"world\" + 0.002*\"stori\" + 0.002*\"life\" + 0.002*\"love\" + 0.002*\"polit\" + 0.002*\"live\" + 0.002*\"year\" + 0.002*\"time\" + 0.002*\"work\"\n",
      "\n",
      "Score: 0.19829292595386505\t \n",
      "Topic: 0.003*\"book\" + 0.002*\"love\" + 0.002*\"life\" + 0.002*\"time\" + 0.002*\"stori\" + 0.002*\"work\" + 0.002*\"world\" + 0.002*\"york\" + 0.002*\"live\" + 0.002*\"peopl\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[1]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
