{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Read in Kaggle df</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path_kaggle = r'C:\\Users\\Desktop\\MSBA Coursework\\Tools for Analytics\\datafile'\\amazon_com_extras.csv'\n",
    "\n",
    "with open(path_kaggle):\n",
    "    #Added fields since line 808 in csv has more than 6 cols and needs to be told to ignore extra data.\n",
    "    \n",
    "    fields=['ASIN','GROUP','FORMAT','TITLE','AUTHOR','PUBLISHER']\n",
    "    df_kaggle = pd.read_csv(path_kaggle, encoding='latin1', dtype=str,usecols=fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Read in amazon_com_extras. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random \n",
    "import json\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "\n",
    "my_csvfile=r'C:\\Users\\Desktop\\MSBA Coursework\\Tools for Analytics\\datafile\\AZscrape.csv'\n",
    "start_entry=48001\n",
    "end_entry=63000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Df_read = df_kaggle.iloc[start_entry:end_entry+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Use ISBN 10 to scrape book descriptions</h3>\n",
    "User Agent Code - Refer https://www.scrapehero.com/how-to-fake-and-rotate-user-agents-using-python-3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "def amazon_description(book_metadata):\n",
    "    \n",
    "    import random\n",
    "    import time\n",
    "    \n",
    "    #New columns that will be added to book_metadata\n",
    "    book_descriptions=list()\n",
    "    book_price=list()\n",
    "    book_rating=list()\n",
    "    \n",
    "    user_agent_list = [\n",
    "           #Chrome\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "            #Firefox\n",
    "            'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)',\n",
    "            'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)',\n",
    "            'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)',\n",
    "            'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)'\n",
    "        ]\n",
    "    \n",
    "    \n",
    "    for i in range(len(book_metadata)):\n",
    "        \n",
    "        #User Agents to prevent bot detection\n",
    "        user_agent=random.choice(user_agent_list)\n",
    "        book_isbn=book_metadata['ASIN'].iloc[i]\n",
    "        \n",
    "        description=''\n",
    "        price=''\n",
    "        rating=''\n",
    "        print(book_isbn)\n",
    "        base_url = \"https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Dstripbooks-intl-ship&field-keywords=\"\n",
    "        url=base_url+book_isbn\n",
    "        print(url)\n",
    "        headers={'User-Agent':user_agent}\n",
    "        book_response=requests.get(url,headers=headers)\n",
    "        while book_response.status_code != 200:\n",
    "            user_agent=random.choice(user_agent_list)\n",
    "            headers={'User-Agent':user_agent}\n",
    "            book_response=requests.get(url,headers=headers)\n",
    "\n",
    "        result_page=bs(book_response.content,'lxml')\n",
    "        all_book_a_tags=result_page.find(\"a\",class_=\"a-link-normal s-access-detail-page s-color-twister-title-link a-text-normal\")\n",
    "        try:\n",
    "            book_url=all_book_a_tags.get(\"href\")\n",
    "            description,price,rating=get_book_description(book_url,user_agent_list)\n",
    "        except: \n",
    "            continue\n",
    "#         print(description,price,rating)\n",
    "        book_descriptions.append(description)\n",
    "        book_price.append(price)\n",
    "        book_rating.append(rating)\n",
    "        \n",
    "#     book_metadata['Description'].iloc[i]=description\n",
    "#     book_metadata['Price'].iloc[i]=price\n",
    "#     book_metadata['Rating'].iloc[i]=rating\n",
    "#     book_metadata.to_csv(my_csvfile,encoding='utf-8') #save data after each query\n",
    "    book_metadata['Description']=book_descriptions\n",
    "    book_metadata['Price']=book_price\n",
    "    book_metadata['Rating']=book_rating\n",
    "    \n",
    "    return book_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_book_description(url,user_agent_list):\n",
    "    book_description=''\n",
    "    book_rating=''\n",
    "    book_price=''\n",
    "    print(url)\n",
    "    #User Agents to prevent bot detection\n",
    "    user_agent = random.choice(user_agent_list)\n",
    "    headers = {'User-Agent': user_agent}\n",
    "    #Throttling to spoof the server into believe these are generated by users\n",
    "    time.sleep(10)\n",
    "    \n",
    "    book_description_response=requests.get(url,headers=headers)\n",
    "    #print(book_description_response.status_code)\n",
    "    while book_description_response.status_code!=200:\n",
    "        print(\"Error in scraping description\")\n",
    "        user_agent = random.choice(user_agent_list)\n",
    "        headers = {'User-Agent': user_agent}\n",
    "        book_description_response=requests.get(url,headers=headers)\n",
    "    \n",
    "    book_page=bs(book_description_response.content,\"lxml\")\n",
    "    a1=book_page.find('div',{'id':'a-page'})\n",
    "    a2=a1.find('div',class_='book en_US').find('div',class_='a-container').find('div',{'id':'centerCol'})\n",
    "    try:\n",
    "        book_description=a2.find('div',{'id':'bookDescription_feature_div'}).find('noscript').find('div').get_text()\n",
    "    except:\n",
    "        book_description=np.NAN\n",
    "    try:\n",
    "        book_rating=a2.find('div',{'id':'averageCustomerReviews_feature_div'}).find('span',class_='a-declarative').find('span',{'id':'acrPopover'}).find('span',class_='a-declarative').find('a').find('i').find('span').get_text().split()[0]    \n",
    "    except:\n",
    "        book_rating=np.NAN\n",
    "    try:\n",
    "        book_price=a2.find('div',{'id':'MediaMatrix'}).find('div').find('div',{'id':'tmmSwatches'}).find('ul').find('li',class_='swatchElement selected').find('span').find('span').find('span').find('span',class_='a-color-base').find('span').get_text().strip()\n",
    "    except:\n",
    "        book_price=np.NAN\n",
    "    return book_description,book_price,book_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_final=amazon_description(Df_read) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Combine in all AZscrape batches 1-7</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path_dir = r'C:\\Users\\sanjayshah\\MSBA Coursework\\Tools for Analytics\\datafile'\n",
    "file_names = ['batch1.csv', 'batch2.csv', 'batch3.csv', 'batch4.csv', 'batch5.csv', 'batch6.csv', 'batch7.csv']\n",
    "file_name_out = 'df_processed.csv'\n",
    "path_output = os.path.join(path_dir, file_name_out)\n",
    "fout = open(path_output, 'w', encoding='utf-8')\n",
    "\n",
    "for batch in file_names:\n",
    "    file_input = os.path.join(path_dir,batch)\n",
    "    fin = open(file_input,'r',encoding='utf-8')\n",
    "    for line in fin:\n",
    "        fout.write(line)\n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Read in df_processed</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "patha =  r'/Users/LovelyCheuk/GithubReps/tools_project/Tools_Analytics/df_processed.csv'\n",
    "with open(patha):\n",
    "    df=pd.read_csv(patha)\n",
    "df = df.dropna(how='any',axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> convert df['Rating'] values to floats and strings to 0's</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ratings_to_floats(df):\n",
    "    import re\n",
    "    for i in df['Rating']:\n",
    "        i = i.strip()\n",
    "        m = bool(re.search('[a-zA-Z]',i))\n",
    "        if i =='' or m==True:\n",
    "            i='0.0'\n",
    "        i = float(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prices_to_floats(df):\n",
    "    import re\n",
    "    for i in df['Price']:\n",
    "        i = i[1:]\n",
    "        i = float(i)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Adding Word Cloud Segment</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color='red'>Please note</font> : This code has been tested on a subset</h3>\n",
    "\n",
    "<h3>Remove the following words -</h3>\n",
    "<li>Short Words</li>\n",
    "<li>Redundant Words that appear in all descriptions</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#MIN_LENGTH and DELETE_WORDS will need to be updated once we identify the concerned words.\n",
    "\n",
    "MIN_LENGTH=5\n",
    "DELETE_WORDS=[]\n",
    "\n",
    "def remove_words(text_string,DELETE_WORDS=DELETE_WORDS):\n",
    "    for word in DELETE_WORDS:\n",
    "        text_string = text_string.replace(word,' ')\n",
    "    return text_string\n",
    "\n",
    "\n",
    "def remove_short_words(text_string,min_length = MIN_LENGTH):\n",
    "    word_list = text_string.split()\n",
    "    for word in word_list:\n",
    "        if len(word) < min_length:\n",
    "            text_string = text_string.replace(' '+word+' ',' ',1)\n",
    "    return text_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Categorize data</h3>\n",
    "\n",
    "<h2>We're trying to group data into the following buckets based on the rating the book has been assigned</h2>\n",
    "\n",
    "<h3><font color='red'>Group 1</font> : Ratings 5 - 4.5</h3>\n",
    "<h3><font color='red'>Group 2</font> : Ratings 4.5 - 4</h3>\n",
    "<h3><font color='red'>Group 3</font> : Ratings 4 - 3.5</h3>\n",
    "<h3><font color='red'>Group 4</font> : Ratings 3.5 - 3</h3>\n",
    "<h3><font color='red'>Group 5</font> : Ratings 3 - 2.5</h3>\n",
    "<h3><font color='red'>Group 0</font> : Irrelevant to the analysis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorize_data(df):\n",
    "    \n",
    "    #This list stores all the categories\n",
    "    category_list=list()\n",
    "\n",
    "    \n",
    "    #Assign categories to each book\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if df['Rating'].iloc[i]>='4.5':\n",
    "            category_list.append(1)\n",
    "        elif df['Rating'].iloc[i]<'4.5' and df['Rating'].iloc[i]>='4':\n",
    "            category_list.append(2)\n",
    "        elif df['Rating'].iloc[i]<'4' and df['Rating'].iloc[i]>='3.5':\n",
    "            category_list.append(3)\n",
    "        elif df['Rating'].iloc[i]<'3.5' and df['Rating'].iloc[i]>='3':\n",
    "            category_list.append(4)\n",
    "        elif df['Rating'].iloc[i]<'3' and df['Rating'].iloc[i]>='2.5':\n",
    "            category_list.append(5)\n",
    "        else:\n",
    "            category_list.append(0)\n",
    "\n",
    "    #Add categories to the DF\n",
    "    df['Category']=category_list\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Code to aggregate text per rating</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggregate_text(temp):\n",
    "    x=''\n",
    "    for i in range(len(temp)):\n",
    "        x=x+' '+temp['Description'].iloc[i]\n",
    "    return x.lower() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Control Loop</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "book_mask = np.array(Image.open(\"book.png\"))\n",
    "\n",
    "df = pd.read_csv('file1.csv')\n",
    "df=df.dropna(how='any',axis=0)\n",
    "\n",
    "COL_NUM = 2\n",
    "ROW_NUM = 3\n",
    "fig, axes = plt.subplots(ROW_NUM, COL_NUM, figsize=(15,15))\n",
    "\n",
    "\n",
    "\n",
    "df=categorize_data(df)\n",
    "\n",
    "#These two lists are for LSI Modeling\n",
    "rating_list=list()\n",
    "rating_description=list()\n",
    "\n",
    "#Please refer to LSI Modeling.ipynb for more details. Code needs to be integrated.\n",
    "\n",
    "for i in sorted(df['Category'].unique()):\n",
    "    temp=df[df['Category']==i]\n",
    "    text=aggregate_text(temp)\n",
    "    text=remove_words(text)\n",
    "    text=remove_short_words(text)\n",
    "    rating_list.append(i)\n",
    "    rating_description.append(text)\n",
    "    ax = axes[i//2, i%2] \n",
    "    ax.set_title(i)\n",
    "    wordcloud = WordCloud(stopwords=STOPWORDS,background_color='white',width=1200,height=1000,max_words=30,contour_width=20,mask=book_mask,contour_color='black').generate(text)\n",
    "    ax.imshow(wordcloud)\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Chart Distribution of Rating</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chart_rating_distribution(df):\n",
    "    %matplotlib inline\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    plt.style.use('seaborn')\n",
    "    from scipy import stats\n",
    "\n",
    "    sns.distplot(df['Rating'],fit=stats.gamma)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Top 15 Top and Bottom Publishers by Rating</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = r'C:\\Users\\endwy\\Documents\\Columbia MSBA\\Fall 2018\\E4501 - Tools for Analytics\\Project\\repo v3\\df_processed.csv'\n",
    "with open(path):\n",
    "    import numpy as np\n",
    "    df = pd.read_csv(path,keep_default_na=False)   \n",
    "    \n",
    "def table_top_and_bottom_publishers_by_rating(df):\n",
    "    bottom = df.groupby('PUBLISHER', as_index=False)['Rating'].mean().nsmallest(15,'Rating')\n",
    "    top = df.groupby('PUBLISHER', as_index=False)['Rating'].mean().nlargest(15,'Rating')\n",
    "    return top,bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Number of Books per Publisher (Top 15)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num_books_per_publisher(df):\n",
    "    from collections import Counter\n",
    "    counts = dict(Counter(df['PUBLISHER']).most_common(15))\n",
    "    pubs = list(counts.keys())\n",
    "    nums = list(counts.values())\n",
    "\n",
    "    import matplotlib.pyplot as plt \n",
    "    %matplotlib inline\n",
    "    import numpy as np\n",
    "    y_pos = np.arange(len(pubs))\n",
    "    plt.barh(y_pos,nums)\n",
    "    plt.title('Number of Books per Publisher (Top 15)')\n",
    "    plt.yticks(y_pos,pubs)\n",
    "\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Proportion of books in each Category(as per rating)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": true
       },
       "data": [
        {
         "labels": [
          0,
          1,
          2,
          3,
          4,
          5
         ],
         "type": "pie",
         "uid": "bc1119fc-e574-4b7f-a733-4a418f01045b",
         "values": [
          28,
          2014,
          1229,
          354,
          117,
          21
         ]
        }
       ],
       "layout": {
        "margin": {
         "b": 40,
         "l": 80,
         "r": 100,
         "t": 40
        },
        "title": "Pie of Category"
       }
      },
      "text/html": [
       "<div id=\"272c2d1c-4196-48da-8412-8813924a43f5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"272c2d1c-4196-48da-8412-8813924a43f5\", [{\"labels\": [0, 1, 2, 3, 4, 5], \"values\": [28, 2014, 1229, 354, 117, 21], \"type\": \"pie\", \"uid\": \"bc1119fc-e574-4b7f-a733-4a418f01045b\"}], {\"margin\": {\"b\": 40, \"l\": 80, \"r\": 100, \"t\": 40}, \"title\": \"Pie of Category\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"272c2d1c-4196-48da-8412-8813924a43f5\"));});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"272c2d1c-4196-48da-8412-8813924a43f5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"272c2d1c-4196-48da-8412-8813924a43f5\", [{\"labels\": [0, 1, 2, 3, 4, 5], \"values\": [28, 2014, 1229, 354, 117, 21], \"type\": \"pie\", \"uid\": \"bc1119fc-e574-4b7f-a733-4a418f01045b\"}], {\"margin\": {\"b\": 40, \"l\": 80, \"r\": 100, \"t\": 40}, \"title\": \"Pie of Category\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"})});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){window._Plotly.Plots.resize(document.getElementById(\"272c2d1c-4196-48da-8412-8813924a43f5\"));});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1=df.copy()\n",
    "df1=categorize_data(df1)\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "def draw_Pie(dataframe, name):\n",
    "    \n",
    "    \n",
    "    tmp = dataframe.groupby(name).size()\n",
    "\n",
    "    # draw histgram\n",
    "    data = [go.Pie(values = tmp.values, \n",
    "               labels = tmp.index)]\n",
    "\n",
    "    # add title\n",
    "    layout = go.Layout(title = 'Pie of ' + name,\n",
    "                      margin = dict(l = 80, r = 100, t = 40, b = 40))\n",
    "\n",
    "    # draw fig\n",
    "    fig = go.Figure(data = data, layout = layout)\n",
    "\n",
    "    iplot(fig, filename = name)\n",
    "draw_Pie(df1, \"Category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color='red'>Note: </font color='red'>The following is extra code that is not to be run atm </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Running the following will convert .ipynb to .py </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script AZscraping.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_aiipp(link):\n",
    "    \"\"\"Returns lists of Author, ISBN-10, ISBN-13, platform, price, category1, category2, description for AZ best seller list books\"\"\"\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    page = requests.get(link)\n",
    "    results_page = BeautifulSoup(page.content,'lxml')\n",
    "    \n",
    "    m = results_page.find('a', class_='a-link-normal contributorNameID').get_text()\n",
    "    book_authors.append(m)\n",
    "    n = results_page.find('div', class_='content').find_all('li')[5].get_text().replace('ISBN-10:','').strip()\n",
    "    book_isbn10.append(n)\n",
    "    p = results_page.find('div', class_='content').find_all('li')[6].get_text().replace('ISBN-13:','').strip()\n",
    "    book_isbn13.append(p)\n",
    "    q = int(results_page.find_all('span',class_='a-size-base')[1].get_text().replace('customer reviews','').strip())\n",
    "    book_num_customer_reviews.append(q)\n",
    "    s = float(results_page.find_all('span',class_='p13n-sc-price')[1].get_text()[1:])\n",
    "    book_prices.append(s)\n",
    "    t = results_page.find('div',class_='a-subheader a-breadcrumb feature').find_all('li')[2].find('a',class_='a-link-normal a-color-tertiary').get_text().replace('\\n','').strip()\n",
    "    book_category1.append(t)\n",
    "    u = results_page.find('div',class_='a-subheader a-breadcrumb feature').find_all('li')[4].find('a',class_='a-link-normal a-color-tertiary').get_text().replace('\\n','').strip()\n",
    "    book_category2.append(u)\n",
    "    \n",
    "    #v = @Akshay to insert description code\n",
    "    #book_description.append(v)\n",
    "    \n",
    "    from time import sleep\n",
    "    sleep(15)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "\n",
    "link_home = 'https://www.amazon.com/best-sellers-books-Amazon/zgbs/books'\n",
    "page = requests.get(link_home)\n",
    "results_page = BeautifulSoup(page.content,'lxml')\n",
    "all_links = results_page.find_all('li',class_='zg-item-immersion')\n",
    "\n",
    "book_links = list()\n",
    "book_titles = list()\n",
    "book_ratings = list()\n",
    "book_authors = list()\n",
    "book_isbn10 = list()\n",
    "book_isbn13 = list()\n",
    "book_num_customer_reviews = list()\n",
    "book_prices = list()\n",
    "book_category1 = list()\n",
    "book_category2 = list()\n",
    "book_description = list()\n",
    "\n",
    "for i in all_links:\n",
    "    m = 'https://www.amazon.com%s' %(i.find('a', class_='a-link-normal').get('href'))\n",
    "    book_links.append(m)\n",
    "    \n",
    "for i in all_links:\n",
    "    m = i.find('div',class_='p13n-sc-truncate p13n-sc-line-clamp-1').get_text()\n",
    "    m = m.replace('\\n','')\n",
    "    m = m.strip()\n",
    "    book_titles.append(m)\n",
    "\n",
    "for i in all_links:\n",
    "    m = i.find_all('a',class_='a-link-normal')[1].get_text()\n",
    "    if '$' in m:\n",
    "        book_ratings.append('No Rating')\n",
    "    else:\n",
    "        m = m.replace('\\n','')\n",
    "        m = m.strip()\n",
    "        m = m[:3]\n",
    "        book_ratings.append(m)\n",
    "        \n",
    "        \n",
    "for i in book_links:\n",
    "    get_aiipp(i)\n",
    "\n",
    "    \n",
    "zip_az = list(zip(book_isbn13,book_isbn10,book_titles,book_authors,book_ratings,book_prices,book_category1,\n",
    "         book_category2,book_num_customer_reviews,book_links,book_description))\n",
    "labels = ['ISBN-13','ISBN-10','title','author','rating','price','category1','category2','num_reviews','link','description']\n",
    "df_az = pd.DataFrame.from_records(zip_az,columns=labels)\n",
    "df_az.rename(columns={'ISBN-13':'ASIN'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
