{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Read in Kaggle df</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path_kaggle = r'C:\\Users\\Desktop\\MSBA Coursework\\Tools for Analytics\\datafile'\\amazon_com_extras.csv'\n",
    "\n",
    "with open(path_kaggle):\n",
    "    #Added fields since line 808 in csv has more than 6 cols and needs to be told to ignore extra data.\n",
    "    \n",
    "    fields=['ASIN','GROUP','FORMAT','TITLE','AUTHOR','PUBLISHER']\n",
    "    df_kaggle = pd.read_csv(path_kaggle, encoding='latin1', dtype=str,usecols=fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Read in amazon_com_extras. </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random \n",
    "import json\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "\n",
    "my_csvfile=r'C:\\Users\\Desktop\\MSBA Coursework\\Tools for Analytics\\datafile\\AZscrape.csv'\n",
    "start_entry=48001\n",
    "end_entry=63000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_read = df_kaggle.iloc[start_entry:end_entry+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Use ISBN 10 to scrape book descriptions</h3>\n",
    "User Agent Code - Refer https://www.scrapehero.com/how-to-fake-and-rotate-user-agents-using-python-3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "def amazon_description(book_metadata):\n",
    "    \n",
    "    import random\n",
    "    import time\n",
    "    \n",
    "    #New columns that will be added to book_metadata\n",
    "    book_descriptions=list()\n",
    "    book_price=list()\n",
    "    book_rating=list()\n",
    "    \n",
    "    user_agent_list = [\n",
    "           #Chrome\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "            #Firefox\n",
    "            'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)',\n",
    "            'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)',\n",
    "            'Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko',\n",
    "            'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)',\n",
    "            'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)',\n",
    "            'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)'\n",
    "        ]\n",
    "    \n",
    "    \n",
    "    book_metadata['Description']=''\n",
    "    book_metadata['Price']=''\n",
    "    book_metadata['Rating']=''\n",
    "    for i in range(len(book_metadata)):\n",
    "        \n",
    "        #User Agents to prevent bot detection\n",
    "        user_agent=random.choice(user_agent_list)\n",
    "        book_isbn=book_metadata['ASIN'].iloc[i]\n",
    "        \n",
    "        print(book_isbn)\n",
    "        base_url = \"https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Dstripbooks-intl-ship&field-keywords=\"\n",
    "        url=base_url+book_isbn\n",
    "        print(url)\n",
    "        headers={'User-Agent':user_agent}\n",
    "        book_response=requests.get(url,headers=headers)\n",
    "        if book_response.status_code != 200:\n",
    "            print(\"Getting Book Details Failed\")\n",
    "            return None\n",
    "        else:\n",
    "            result_page=bs(book_response.content,'lxml')\n",
    "            all_book_a_tags=result_page.find(\"a\",class_=\"a-link-normal s-access-detail-page s-color-twister-title-link a-text-normal\")\n",
    "            try:\n",
    "                book_url=all_book_a_tags.get(\"href\")\n",
    "                description,price,rating=get_book_description(book_url,user_agent_list)\n",
    "            except: \n",
    "                continue\n",
    "            print(description,price,rating)\n",
    "#             book_descriptions.append(description)\n",
    "#             book_price.append(price)\n",
    "#             book_rating.append(rating)\n",
    "        book_metadata['Description'].iloc[i]=description\n",
    "        book_metadata['Price'].iloc[i]=price\n",
    "        book_metadata['Rating'].iloc[i]=rating\n",
    "        book_metadata.to_csv(my_csvfile,encoding='utf-8') #save data after each query\n",
    "#     book_metadata['Description']=book_descriptions\n",
    "#     book_metadata['Price']=book_price\n",
    "#     book_metadata['Rating']=book_rating\n",
    "    \n",
    "    return book_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_description(url,user_agent_list):\n",
    "    book_description=''\n",
    "    book_rating=''\n",
    "    book_price=''\n",
    "    print(url)\n",
    "    #User Agents to prevent bot detection\n",
    "    user_agent = random.choice(user_agent_list)\n",
    "    headers = {'User-Agent': user_agent}\n",
    "    \n",
    "    #Throttling to spoof the server into believe these are generated by users\n",
    "    time.sleep(7)\n",
    "    \n",
    "    book_description_response=requests.get(url,headers=headers)\n",
    "    if book_description_response.status_code!=200:\n",
    "        print(\"Error in scraping description\")\n",
    "        return None\n",
    "    else:\n",
    "        book_page=bs(book_description_response.content,\"lxml\")\n",
    "        a1=book_page.find('div',{'id':'a-page'})\n",
    "        a2=a1.find('div',class_='book en_US').find('div',class_='a-container').find('div',{'id':'centerCol'})\n",
    "        try:\n",
    "            book_description=a2.find('div',{'id':'bookDescription_feature_div'}).find('noscript').find('div').get_text()\n",
    "        except:\n",
    "            book_description=np.NAN\n",
    "        try:\n",
    "            book_rating=a2.find('div',{'id':'averageCustomerReviews_feature_div'}).find('span',class_='a-declarative').find('span',{'id':'acrPopover'}).find('span',class_='a-declarative').find('a').find('i').find('span').get_text().split()[0]    \n",
    "        except:\n",
    "            book_rating=np.NAN\n",
    "        try:\n",
    "            book_price=a2.find('div',{'id':'MediaMatrix'}).find('div').find('div',{'id':'tmmSwatches'}).find('ul').find('li',class_='swatchElement selected').find('span').find('span').find('span').find('span',class_='a-color-base').find('span').get_text().strip()\n",
    "        except:\n",
    "            book_price=np.NAN\n",
    "        return book_description,book_price,book_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=amazon_description(Df_read) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color='red'>Note: </font color='red'>Below code not vetted - please do not delete </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Read in all AZscrape files</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path1 = r'C:\\Users\\sanjayshah\\Desktop\\MSBA Coursework\\Tools for Analytics\\datafile\\batch1.csv'\n",
    "path2 = r'C:\\Users\\sanjayshah\\Desktop\\MSBA Coursework\\Tools for Analytics\\datafile\\batch2.csv'\n",
    "path3 =r'C:\\Users\\sanjayshah\\Desktop\\SBA Coursework\\Tools for Analytics\\datafile\\batch3.csv'\n",
    "path4 = r'C:\\Users\\sanjayshah\\Desktop\\MSBA Coursework\\Tools for Analytics\\datafile\\batch4.csv'\n",
    "path5 = r'C:\\Users\\sanjayshah\\Desktop\\MSBA Coursework\\Tools for Analytics\\datafile\\batch5.csv'\n",
    "path6 = r'C:\\Users\\sanjayshah\\Desktop\\MSBA Coursework\\Tools for Analytics\\datafile\\batch6.csv'\n",
    "path7 = r'C:\\Users\\sanjayshah\\Desktop\\MSBA Coursework\\Tools for Analytics\\datafile\\batch7.csv'\n",
    "\n",
    "with open(path1):\n",
    "    batch1 = pd.read_csv(path1)\n",
    "with open(path2):\n",
    "    batch2 = pd.read_csv(path2)\n",
    "with open(path3):\n",
    "    batch3 = pd.read_csv(path3)\n",
    "with open(path4):\n",
    "    batch4 = pd.read_csv(path4)\n",
    "with open(path5):\n",
    "    batch5 = pd.read_csv(path5)\n",
    "with open(path6):\n",
    "    batch6 = pd.read_csv(path6)\n",
    "with open(path7):\n",
    "    batch7 = pd.read_csv(path7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Combine batches 1-7</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path_dir = r'C:\\Users\\sanjayshah\\MSBA Coursework\\Tools for Analytics\\datafile'\n",
    "file_names = ['batch1.csv', 'batch2.csv', 'batch3.csv', 'batch4.csv', 'batch5.csv', 'batch6.csv', 'batch7.csv']\n",
    "file_name_out = 'df_processed.csv'\n",
    "path_output = os.path.join(path_dir, file_name_out)\n",
    "fout = open(path_output, 'w', encoding='utf-8')\n",
    "\n",
    "for batch in file_names:\n",
    "    file_input = os.path.join(path_dir,batch)\n",
    "    fin = open(file_input,'r',encoding='utf-8')\n",
    "    for line in fin:\n",
    "        fout.write(line)\n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Read in df_processed</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patha = r'C:\\Users\\sanjayshah\\MSBA Coursework\\Tools for Analytics\\datafile'\n",
    "with open(patha):\n",
    "    df=pd.read_csv(patha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Adding Word Cloud Segment</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color='red'>Please note</font> : This code has been tested on a subset</h3>\n",
    "\n",
    "<h3>Remove the following words -</h3>\n",
    "<li>Short Words</li>\n",
    "<li>Redundant Words that appear in all descriptions</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#MIN_LENGTH and DELETE_WORDS will need to be updated once we identify the concerned words.\n",
    "\n",
    "MIN_LENGTH=5\n",
    "DELETE_WORDS=[]\n",
    "\n",
    "def remove_words(text_string,DELETE_WORDS=DELETE_WORDS):\n",
    "    for word in DELETE_WORDS:\n",
    "        text_string = text_string.replace(word,' ')\n",
    "    return text_string\n",
    "\n",
    "\n",
    "def remove_short_words(text_string,min_length = MIN_LENGTH):\n",
    "    word_list = text_string.split()\n",
    "    for word in word_list:\n",
    "        if len(word) < min_length:\n",
    "            text_string = text_string.replace(' '+word+' ',' ',1)\n",
    "    return text_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Categorize data</h3>\n",
    "\n",
    "<h2>We're trying to group data into the following buckets based on the rating the book has been assigned</h2>\n",
    "\n",
    "<h3><font color='red'>Group 1</font> : Ratings 5 - 4.5</h3>\n",
    "<h3><font color='red'>Group 2</font> : Ratings 4.5 - 4</h3>\n",
    "<h3><font color='red'>Group 3</font> : Ratings 4 - 3.5</h3>\n",
    "<h3><font color='red'>Group 4</font> : Ratings 3.5 - 3</h3>\n",
    "<h3><font color='red'>Group 5</font> : Ratings 3 - 2.5</h3>\n",
    "<h3><font color='red'>Group 0</font> : Irrelevant to the analysis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_data(df):\n",
    "    \n",
    "    #This list stores all the categories\n",
    "    category_list=list()\n",
    "\n",
    "    \n",
    "    #Assign categories to each book\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if df['Rating'].iloc[i]>=4.5:\n",
    "            category_list.append(1)\n",
    "        elif df['Rating'].iloc[i]<4.5 and df['Rating'].iloc[i]>=4:\n",
    "            category_list.append(2)\n",
    "        elif df['Rating'].iloc[i]<4 and df['Rating'].iloc[i]>=3.5:\n",
    "            category_list.append(3)\n",
    "        elif df['Rating'].iloc[i]<3.5 and df['Rating'].iloc[i]>=3:\n",
    "            category_list.append(4)\n",
    "        elif df['Rating'].iloc[i]<3 and df['Rating'].iloc[i]>=2.5:\n",
    "            category_list.append(5)\n",
    "        else:\n",
    "            category_list.append(0)\n",
    "\n",
    "    #Add categories to the DF\n",
    "    df['Category']=category_list\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color='red'>Note: </font color='red'>The following is extra code that is not to be run atm </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Running the following will convert .ipynb to .py </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script AZscraping.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aiipp(link):\n",
    "    \"\"\"Returns lists of Author, ISBN-10, ISBN-13, platform, price, category1, category2, description for AZ best seller list books\"\"\"\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    page = requests.get(link)\n",
    "    results_page = BeautifulSoup(page.content,'lxml')\n",
    "    \n",
    "    m = results_page.find('a', class_='a-link-normal contributorNameID').get_text()\n",
    "    book_authors.append(m)\n",
    "    n = results_page.find('div', class_='content').find_all('li')[5].get_text().replace('ISBN-10:','').strip()\n",
    "    book_isbn10.append(n)\n",
    "    p = results_page.find('div', class_='content').find_all('li')[6].get_text().replace('ISBN-13:','').strip()\n",
    "    book_isbn13.append(p)\n",
    "    q = int(results_page.find_all('span',class_='a-size-base')[1].get_text().replace('customer reviews','').strip())\n",
    "    book_num_customer_reviews.append(q)\n",
    "    s = float(results_page.find_all('span',class_='p13n-sc-price')[1].get_text()[1:])\n",
    "    book_prices.append(s)\n",
    "    t = results_page.find('div',class_='a-subheader a-breadcrumb feature').find_all('li')[2].find('a',class_='a-link-normal a-color-tertiary').get_text().replace('\\n','').strip()\n",
    "    book_category1.append(t)\n",
    "    u = results_page.find('div',class_='a-subheader a-breadcrumb feature').find_all('li')[4].find('a',class_='a-link-normal a-color-tertiary').get_text().replace('\\n','').strip()\n",
    "    book_category2.append(u)\n",
    "    \n",
    "    #v = @Akshay to insert description code\n",
    "    #book_description.append(v)\n",
    "    \n",
    "    from time import sleep\n",
    "    sleep(15)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "\n",
    "link_home = 'https://www.amazon.com/best-sellers-books-Amazon/zgbs/books'\n",
    "page = requests.get(link_home)\n",
    "results_page = BeautifulSoup(page.content,'lxml')\n",
    "all_links = results_page.find_all('li',class_='zg-item-immersion')\n",
    "\n",
    "book_links = list()\n",
    "book_titles = list()\n",
    "book_ratings = list()\n",
    "book_authors = list()\n",
    "book_isbn10 = list()\n",
    "book_isbn13 = list()\n",
    "book_num_customer_reviews = list()\n",
    "book_prices = list()\n",
    "book_category1 = list()\n",
    "book_category2 = list()\n",
    "book_description = list()\n",
    "\n",
    "for i in all_links:\n",
    "    m = 'https://www.amazon.com%s' %(i.find('a', class_='a-link-normal').get('href'))\n",
    "    book_links.append(m)\n",
    "    \n",
    "for i in all_links:\n",
    "    m = i.find('div',class_='p13n-sc-truncate p13n-sc-line-clamp-1').get_text()\n",
    "    m = m.replace('\\n','')\n",
    "    m = m.strip()\n",
    "    book_titles.append(m)\n",
    "\n",
    "for i in all_links:\n",
    "    m = i.find_all('a',class_='a-link-normal')[1].get_text()\n",
    "    if '$' in m:\n",
    "        book_ratings.append('No Rating')\n",
    "    else:\n",
    "        m = m.replace('\\n','')\n",
    "        m = m.strip()\n",
    "        m = m[:3]\n",
    "        book_ratings.append(m)\n",
    "        \n",
    "        \n",
    "for i in book_links:\n",
    "    get_aiipp(i)\n",
    "\n",
    "    \n",
    "zip_az = list(zip(book_isbn13,book_isbn10,book_titles,book_authors,book_ratings,book_prices,book_category1,\n",
    "         book_category2,book_num_customer_reviews,book_links,book_description))\n",
    "labels = ['ISBN-13','ISBN-10','title','author','rating','price','category1','category2','num_reviews','link','description']\n",
    "df_az = pd.DataFrame.from_records(zip_az,columns=labels)\n",
    "df_az.rename(columns={'ISBN-13':'ASIN'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
